import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, VotingClassifier
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, mean_absolute_error, mean_squared_error
from sklearn.multioutput import MultiOutputRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def calculate_team_features(df):
    """
    Calculate advanced team features including form, strength, and historical performance
    """
    print("Calculating advanced team features...")
    
    # Sort by date to ensure chronological order
    df = df.sort_values('date').reset_index(drop=True)
    
    # Initialize feature columns
    feature_columns = [
        'home_recent_form', 'away_recent_form',
        'home_avg_goals_for', 'home_avg_goals_against',
        'away_avg_goals_for', 'away_avg_goals_against',
        'home_win_rate', 'away_win_rate',
        'home_strength', 'away_strength',
        'goal_difference_home', 'goal_difference_away',
        'h2h_home_advantage', 'matches_played_home', 'matches_played_away'
    ]
    
    for col in feature_columns:
        df[col] = 0.0
    
    # Calculate features for each match
    for idx in range(len(df)):
        current_match = df.iloc[idx]
        home_team = current_match['home_team']
        away_team = current_match['away_team']
        
        # Get historical data before current match
        history = df.iloc[:idx]
        
        if len(history) > 0:
            # Home team statistics
            home_matches = history[(history['home_team'] == home_team) | (history['away_team'] == home_team)]
            
            if len(home_matches) >= 3:  # Need at least 3 matches for reliable stats
                # Recent form (last 5 matches)
                recent_home = home_matches.tail(5)
                home_form = 0
                home_goals_for = []
                home_goals_against = []
                home_wins = 0
                
                for _, match in recent_home.iterrows():
                    if match['home_team'] == home_team:
                        home_goals_for.append(match['home_goals'])
                        home_goals_against.append(match['away_goals'])
                        if match['result'] == 'H':
                            home_form += 3
                            home_wins += 1
                        elif match['result'] == 'D':
                            home_form += 1
                    else:  # Away match for home team
                        home_goals_for.append(match['away_goals'])
                        home_goals_against.append(match['home_goals'])
                        if match['result'] == 'A':
                            home_form += 3
                            home_wins += 1
                        elif match['result'] == 'D':
                            home_form += 1
                
                df.at[idx, 'home_recent_form'] = home_form / len(recent_home)
                df.at[idx, 'home_avg_goals_for'] = np.mean(home_goals_for) if home_goals_for else 0
                df.at[idx, 'home_avg_goals_against'] = np.mean(home_goals_against) if home_goals_against else 0
                df.at[idx, 'home_win_rate'] = home_wins / len(recent_home)
                df.at[idx, 'matches_played_home'] = len(home_matches)
                
                # Goal difference
                df.at[idx, 'goal_difference_home'] = df.at[idx, 'home_avg_goals_for'] - df.at[idx, 'home_avg_goals_against']
            
            # Away team statistics
            away_matches = history[(history['home_team'] == away_team) | (history['away_team'] == away_team)]
            
            if len(away_matches) >= 3:
                # Recent form (last 5 matches)
                recent_away = away_matches.tail(5)
                away_form = 0
                away_goals_for = []
                away_goals_against = []
                away_wins = 0
                
                for _, match in recent_away.iterrows():
                    if match['home_team'] == away_team:
                        away_goals_for.append(match['home_goals'])
                        away_goals_against.append(match['away_goals'])
                        if match['result'] == 'H':
                            away_form += 3
                            away_wins += 1
                        elif match['result'] == 'D':
                            away_form += 1
                    else:  # Away match for away team
                        away_goals_for.append(match['away_goals'])
                        away_goals_against.append(match['home_goals'])
                        if match['result'] == 'A':
                            away_form += 3
                            away_wins += 1
                        elif match['result'] == 'D':
                            away_form += 1
                
                df.at[idx, 'away_recent_form'] = away_form / len(recent_away)
                df.at[idx, 'away_avg_goals_for'] = np.mean(away_goals_for) if away_goals_for else 0
                df.at[idx, 'away_avg_goals_against'] = np.mean(away_goals_against) if away_goals_against else 0
                df.at[idx, 'away_win_rate'] = away_wins / len(recent_away)
                df.at[idx, 'matches_played_away'] = len(away_matches)
                
                # Goal difference
                df.at[idx, 'goal_difference_away'] = df.at[idx, 'away_avg_goals_for'] - df.at[idx, 'away_avg_goals_against']
            
            # Head-to-head record
            h2h = history[((history['home_team'] == home_team) & (history['away_team'] == away_team)) |
                         ((history['home_team'] == away_team) & (history['away_team'] == home_team))]
            
            if len(h2h) > 0:
                home_h2h_wins = 0
                for _, match in h2h.iterrows():
                    if (match['home_team'] == home_team and match['result'] == 'H') or \
                       (match['away_team'] == home_team and match['result'] == 'A'):
                        home_h2h_wins += 1
                
                df.at[idx, 'h2h_home_advantage'] = home_h2h_wins / len(h2h)
        
        # Team strength based on recent performance
        if df.at[idx, 'matches_played_home'] > 0:
            df.at[idx, 'home_strength'] = (df.at[idx, 'home_recent_form'] + 
                                         df.at[idx, 'goal_difference_home'] + 
                                         df.at[idx, 'home_win_rate'] * 3) / 3
        
        if df.at[idx, 'matches_played_away'] > 0:
            df.at[idx, 'away_strength'] = (df.at[idx, 'away_recent_form'] + 
                                         df.at[idx, 'goal_difference_away'] + 
                                         df.at[idx, 'away_win_rate'] * 3) / 3
    
    return df

def load_and_preprocess_data(filepath):
    """
    Load and preprocess the football dataset with advanced feature engineering
    """
    print("Loading dataset...")
    df = pd.read_excel(filepath)
    
    print(f"Dataset shape: {df.shape}")
    print(f"Columns: {list(df.columns)}")
    
    # Convert date column to datetime
    df['date'] = pd.to_datetime(df['date'])
    
    # Calculate advanced features
    df = calculate_team_features(df)
    
    # Encode team names
    print("\nEncoding team names...")
    home_encoder = LabelEncoder()
    away_encoder = LabelEncoder()
    result_encoder = LabelEncoder()
    
    df['home_team_encoded'] = home_encoder.fit_transform(df['home_team'])
    df['away_team_encoded'] = away_encoder.fit_transform(df['away_team'])
    
    # Define enhanced features
    feature_cols = [
        'home_team_encoded', 'away_team_encoded',
        'home_recent_form', 'away_recent_form',
        'home_avg_goals_for', 'home_avg_goals_against',
        'away_avg_goals_for', 'away_avg_goals_against',
        'home_win_rate', 'away_win_rate',
        'home_strength', 'away_strength',
        'goal_difference_home', 'goal_difference_away',
        'h2h_home_advantage', 'matches_played_home', 'matches_played_away'
    ]
    
    X = df[feature_cols]
    
    # Remove matches with insufficient historical data (first 10 matches)
    valid_matches = df['matches_played_home'] >= 3
    X = X[valid_matches]
    df_filtered = df[valid_matches].reset_index(drop=True)
    
    # Classification target: result (H/A/D) - encode for XGBoost compatibility
    y_class = result_encoder.fit_transform(df_filtered['result'])
    
    # Regression targets: home_goals, away_goals
    y_reg = df_filtered[['home_goals', 'away_goals']]
    
    print(f"\nFiltered dataset shape: {X.shape}")
    print(f"Features: {len(feature_cols)} features including advanced statistics")
    print(f"Classification target distribution:")
    result_counts = pd.Series(y_class).value_counts()
    result_mapping = {i: label for i, label in enumerate(result_encoder.classes_)}
    for encoded_val, count in result_counts.items():
        print(f"  {result_mapping[encoded_val]}: {count}")
    
    return X, y_class, y_reg, home_encoder, away_encoder, result_encoder, df_filtered

def split_data(X, y_class, y_reg, test_size=0.2, random_state=42):
    """
    Split data with smaller test size for more training data
    """
    print(f"\nSplitting data: {int((1-test_size)*100)}% training, {int(test_size*100)}% testing...")
    
    X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
        X, y_class, y_reg, test_size=test_size, random_state=random_state, stratify=y_class
    )
    
    print(f"Training set size: {X_train.shape[0]}")
    print(f"Test set size: {X_test.shape[0]}")
    
    return X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test

def train_advanced_classification_model(X_train, y_train):
    """
    Train an ensemble classification model with hyperparameter tuning
    """
    print("\nTraining advanced ensemble classification model...")
    
    # Scale features for better performance
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    
    try:
        from xgboost import XGBClassifier
        
        # XGBoost with optimized parameters
        xgb_model = XGBClassifier(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='mlogloss'
        )
        print("Using XGBoostClassifier with optimized parameters")
        
        # Random Forest with optimized parameters
        rf_model = RandomForestClassifier(
            n_estimators=150,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        
        # Logistic Regression for diversity
        lr_model = LogisticRegression(
            max_iter=1000,
            random_state=42,
            C=1.0
        )
        
        # Create ensemble
        ensemble = VotingClassifier(
            estimators=[
                ('xgb', xgb_model),
                ('rf', rf_model),
                ('lr', lr_model)
            ],
            voting='soft'
        )
        
        # Fit ensemble on original features (XGBoost handles scaling internally)
        ensemble.fit(X_train, y_train)
        
        return ensemble, None  # No scaler needed for prediction
        
    except ImportError:
        print("XGBoost not available, using optimized Random Forest")
        
        # Grid search for best parameters
        rf_model = RandomForestClassifier(random_state=42)
        
        param_grid = {
            'n_estimators': [100, 150, 200],
            'max_depth': [10, 12, 15],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
        
        grid_search = GridSearchCV(
            rf_model, param_grid, 
            cv=5, scoring='accuracy', 
            n_jobs=-1
        )
        
        grid_search.fit(X_train, y_train)
        print(f"Best parameters: {grid_search.best_params_}")
        
        return grid_search.best_estimator_, None

def train_advanced_regression_model(X_train, y_train):
    """
    Train an advanced regression model with hyperparameter tuning
    """
    print("\nTraining advanced regression model...")
    
    try:
        from xgboost import XGBRegressor
        
        base_model = XGBRegressor(
            n_estimators=200,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
        print("Using optimized XGBoostRegressor")
        
    except ImportError:
        base_model = RandomForestRegressor(
            n_estimators=150,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        print("Using optimized RandomForestRegressor")
    
    model = MultiOutputRegressor(base_model)
    model.fit(X_train, y_train)
    return model

def evaluate_classification_model(model, X_test, y_test, result_encoder):
    """
    Evaluate classification model performance with detailed metrics
    """
    print("\n" + "="*50)
    print("ADVANCED CLASSIFICATION MODEL EVALUATION")
    print("="*50)
    
    # Make predictions
    y_pred = model.predict(X_test)
    y_pred_proba = model.predict_proba(X_test)
    
    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred, average='weighted')
    cm = confusion_matrix(y_test, y_pred)
    
    print(f"Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")
    print(f"F1-Score (weighted): {f1:.4f}")
    
    # Detailed confusion matrix
    print(f"\nConfusion Matrix:")
    print("Predicted ->")
    labels = result_encoder.classes_
    print(f"{'Actual':<8}", end="")
    for label in labels:
        print(f"{label:<6}", end="")
    print()
    
    for i, label in enumerate(labels):
        print(f"{label:<8}", end="")
        for j in range(len(labels)):
            print(f"{cm[i][j]:<6}", end="")
        print()
    
    # Precision and Recall by class
    print(f"\nPer-class Performance:")
    for i, label in enumerate(labels):
        precision = cm[i][i] / (cm[:, i].sum() + 1e-8)
        recall = cm[i][i] / (cm[i, :].sum() + 1e-8)
        print(f"{label}: Precision={precision:.3f}, Recall={recall:.3f}")
    
    return {
        'accuracy': accuracy,
        'f1_score': f1,
        'confusion_matrix': cm,
        'predictions': y_pred,
        'probabilities': y_pred_proba
    }

def evaluate_regression_model(model, X_test, y_test):
    """
    Evaluate regression model with enhanced metrics
    """
    print("\n" + "="*50)
    print("ADVANCED REGRESSION MODEL EVALUATION")
    print("="*50)
    
    y_pred = model.predict(X_test)
    
    # Enhanced metrics
    mae_home = mean_absolute_error(y_test.iloc[:, 0], y_pred[:, 0])
    mae_away = mean_absolute_error(y_test.iloc[:, 1], y_pred[:, 1])
    
    rmse_home = np.sqrt(mean_squared_error(y_test.iloc[:, 0], y_pred[:, 0]))
    rmse_away = np.sqrt(mean_squared_error(y_test.iloc[:, 1], y_pred[:, 1]))
    
    overall_mae = (mae_home + mae_away) / 2
    overall_rmse = (rmse_home + rmse_away) / 2
    
    print(f"Home Goals - MAE: {mae_home:.4f}, RMSE: {rmse_home:.4f}")
    print(f"Away Goals - MAE: {mae_away:.4f}, RMSE: {rmse_away:.4f}")
    print(f"Overall - MAE: {overall_mae:.4f}, RMSE: {overall_rmse:.4f}")
    
    # Score accuracy (exact score predictions)
    exact_scores = 0
    for i in range(len(y_test)):
        pred_home = max(0, round(y_pred[i, 0]))
        pred_away = max(0, round(y_pred[i, 1]))
        actual_home = y_test.iloc[i, 0]
        actual_away = y_test.iloc[i, 1]
        
        if pred_home == actual_home and pred_away == actual_away:
            exact_scores += 1
    
    score_accuracy = exact_scores / len(y_test)
    print(f"Exact Score Accuracy: {score_accuracy:.4f} ({score_accuracy*100:.1f}%)")
    
    return {
        'mae_home': mae_home,
        'mae_away': mae_away,
        'rmse_home': rmse_home,
        'rmse_away': rmse_away,
        'overall_mae': overall_mae,
        'overall_rmse': overall_rmse,
        'score_accuracy': score_accuracy,
        'predictions': y_pred
    }

def make_predictions_for_all_matches(class_model, reg_model, X, original_df, home_encoder, away_encoder, result_encoder):
    """
    Make enhanced predictions for ALL matches with confidence scoring
    """
    print("\n" + "="*50)
    print("ENHANCED PREDICTIONS FOR ALL MATCHES")
    print("="*50)
    
    # Make predictions
    result_pred_encoded = class_model.predict(X)
    result_pred = result_encoder.inverse_transform(result_pred_encoded)
    goals_pred = reg_model.predict(X)
    result_proba = class_model.predict_proba(X)
    
    # Create enhanced results dataframe
    results_df = original_df.copy()
    results_df['predicted_result'] = result_pred
    results_df['predicted_home_goals'] = np.maximum(0, np.round(goals_pred[:, 0])).astype(int)
    results_df['predicted_away_goals'] = np.maximum(0, np.round(goals_pred[:, 1])).astype(int)
    results_df['prediction_confidence'] = np.max(result_proba, axis=1)
    
    # Calculate advanced accuracy metrics
    actual_results = original_df['result'].values
    accuracy = np.mean(actual_results == result_pred)
    
    # High confidence predictions (>80%)
    high_conf_mask = results_df['prediction_confidence'] > 0.8
    high_conf_accuracy = np.mean(actual_results[high_conf_mask] == result_pred[high_conf_mask]) if np.sum(high_conf_mask) > 0 else 0
    
    print(f"Overall Prediction Accuracy: {accuracy:.4f} ({accuracy*100:.1f}%)")
    print(f"High Confidence (>80%) Accuracy: {high_conf_accuracy:.4f} ({high_conf_accuracy*100:.1f}%) - {np.sum(high_conf_mask)} matches")
    
    # Score accuracy
    exact_scores = 0
    for i in range(len(results_df)):
        if (results_df.iloc[i]['predicted_home_goals'] == results_df.iloc[i]['home_goals'] and
            results_df.iloc[i]['predicted_away_goals'] == results_df.iloc[i]['away_goals']):
            exact_scores += 1
    
    score_accuracy = exact_scores / len(results_df)
    print(f"Exact Score Accuracy: {score_accuracy:.4f} ({score_accuracy*100:.1f}%)")
    
    # Show best predictions
    print(f"\nTop 10 Most Confident Predictions:")
    print("-" * 80)
    
    top_predictions = results_df.nlargest(10, 'prediction_confidence')
    
    for i, (_, row) in enumerate(top_predictions.iterrows()):
        actual_score = f"{row['home_goals']}-{row['away_goals']}"
        predicted_score = f"{row['predicted_home_goals']}-{row['predicted_away_goals']}"
        correct = "âœ…" if row['result'] == row['predicted_result'] else "âŒ"
        
        print(f"{i+1:2d}. Home: {row['home_team']}, Away: {row['away_team']} â†’ "
              f"Predicted: {row['predicted_result']}, Score: {predicted_score} "
              f"(Conf: {row['prediction_confidence']:.2%}) {correct}")
        print(f"    Actual: Result={row['result']}, Score={actual_score}")
        print()
    
    # Accuracy by result type
    print("Prediction Accuracy by Result Type:")
    print("-" * 40)
    for result_type in ['H', 'A', 'D']:
        mask = actual_results == result_type
        if np.sum(mask) > 0:
            type_accuracy = np.mean(result_pred[mask] == result_type)
            count = np.sum(mask)
            print(f"{result_type}: {type_accuracy:.4f} ({type_accuracy*100:.1f}%) - {count} matches")
    
    return results_df

def save_models_and_encoders(class_model, reg_model, home_encoder, away_encoder, result_encoder, scaler=None):
    """
    Save trained models and encoders using joblib
    
    Args:
        class_model: Trained classification model
        reg_model: Trained regression model
        home_encoder: LabelEncoder for home teams
        away_encoder: LabelEncoder for away teams
        result_encoder: LabelEncoder for results
        scaler: StandardScaler if used
    """
    print("\n" + "="*50)
    print("ğŸ’¾ SAVING MODELS AND ENCODERS")
    print("="*50)
    
    # Create models directory if it doesn't exist
    models_dir = "football_models"
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
    
    try:
        # Save classification model
        joblib.dump(class_model, os.path.join(models_dir, 'classification_model.pkl'))
        print("âœ… Classification model saved to 'football_models/classification_model.pkl'")
        
        # Save regression model
        joblib.dump(reg_model, os.path.join(models_dir, 'regression_model.pkl'))
        print("âœ… Regression model saved to 'football_models/regression_model.pkl'")
        
        # Save encoders
        joblib.dump(home_encoder, os.path.join(models_dir, 'home_team_encoder.pkl'))
        print("âœ… Home team encoder saved to 'football_models/home_team_encoder.pkl'")
        
        joblib.dump(away_encoder, os.path.join(models_dir, 'away_team_encoder.pkl'))
        print("âœ… Away team encoder saved to 'football_models/away_team_encoder.pkl'")
        
        joblib.dump(result_encoder, os.path.join(models_dir, 'result_encoder.pkl'))
        print("âœ… Result encoder saved to 'football_models/result_encoder.pkl'")
        
        # Save scaler if used
        if scaler is not None:
            joblib.dump(scaler, os.path.join(models_dir, 'feature_scaler.pkl'))
            print("âœ… Feature scaler saved to 'football_models/feature_scaler.pkl'")
        
        # Save model metadata
        metadata = {
            'saved_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'model_type': 'Advanced Ensemble Football Prediction',
            'features_count': 15,
            'has_scaler': scaler is not None,
            'team_classes_home': list(home_encoder.classes_),
            'team_classes_away': list(away_encoder.classes_),
            'result_classes': list(result_encoder.classes_)
        }
        
        joblib.dump(metadata, os.path.join(models_dir, 'model_metadata.pkl'))
        print("âœ… Model metadata saved to 'football_models/model_metadata.pkl'")
        
        print(f"\nğŸ“ All models saved successfully in '{models_dir}' directory!")
        
    except Exception as e:
        print(f"âŒ Error saving models: {str(e)}")

def load_models_and_encoders(models_dir="football_models"):
    """
    Load trained models and encoders using joblib
    
    Args:
        models_dir: Directory containing saved models
    
    Returns:
        tuple: Loaded models and encoders
    """
    print("\n" + "="*50)
    print("ğŸ“‚ LOADING SAVED MODELS AND ENCODERS")
    print("="*50)
    
    try:
        # Load models
        class_model = joblib.load(os.path.join(models_dir, 'classification_model.pkl'))
        print("âœ… Classification model loaded")
        
        reg_model = joblib.load(os.path.join(models_dir, 'regression_model.pkl'))
        print("âœ… Regression model loaded")
        
        # Load encoders
        home_encoder = joblib.load(os.path.join(models_dir, 'home_team_encoder.pkl'))
        print("âœ… Home team encoder loaded")
        
        away_encoder = joblib.load(os.path.join(models_dir, 'away_team_encoder.pkl'))
        print("âœ… Away team encoder loaded")
        
        result_encoder = joblib.load(os.path.join(models_dir, 'result_encoder.pkl'))
        print("âœ… Result encoder loaded")
        
        # Load scaler if exists
        scaler = None
        scaler_path = os.path.join(models_dir, 'feature_scaler.pkl')
        if os.path.exists(scaler_path):
            scaler = joblib.load(scaler_path)
            print("âœ… Feature scaler loaded")
        
        # Load metadata
        metadata = joblib.load(os.path.join(models_dir, 'model_metadata.pkl'))
        print("âœ… Model metadata loaded")
        
        print(f"\nğŸ“Š Model Information:")
        print(f"   Saved Date: {metadata['saved_date']}")
        print(f"   Model Type: {metadata['model_type']}")
        print(f"   Features Count: {metadata['features_count']}")
        print(f"   Teams Available: {len(metadata['team_classes_home'])} teams")
        
        return class_model, reg_model, home_encoder, away_encoder, result_encoder, scaler, metadata
        
    except FileNotFoundError as e:
        print(f"âŒ Model files not found. Please train and save models first.")
        print(f"Missing file: {str(e)}")
        return None, None, None, None, None, None, None
    except Exception as e:
        print(f"âŒ Error loading models: {str(e)}")
        return None, None, None, None, None, None, None

def predict_new_matches(home_teams, away_teams, class_model, reg_model, home_encoder, away_encoder, result_encoder):
    """
    Make predictions for new matches using loaded models
    
    Args:
        home_teams: List of home team names
        away_teams: List of away team names
        class_model, reg_model: Loaded models
        home_encoder, away_encoder, result_encoder: Loaded encoders
    
    Returns:
        DataFrame with predictions
    """
    print("\n" + "="*50)
    print("ğŸ”® MAKING NEW PREDICTIONS")
    print("="*50)
    
    try:
        # Create basic features (for demonstration - in practice, you'd need historical data)
        predictions_data = []
        
        for home_team, away_team in zip(home_teams, away_teams):
            # Encode team names
            try:
                home_encoded = home_encoder.transform([home_team])[0]
                away_encoded = away_encoder.transform([away_team])[0]
            except ValueError as e:
                print(f"âŒ Unknown team: {e}")
                continue
            
            # For demonstration, use average values for other features
            # In practice, you'd calculate these from recent match data
            features = [
                home_encoded, away_encoded,
                1.5, 1.3,  # home_recent_form, away_recent_form
                1.8, 1.0,  # home_avg_goals_for, home_avg_goals_against
                1.2, 1.4,  # away_avg_goals_for, away_avg_goals_against
                0.6, 0.4,  # home_win_rate, away_win_rate
                1.2, 0.8,  # home_strength, away_strength
                0.8, -0.2, # goal_difference_home, goal_difference_away
                0.5, 10, 8 # h2h_home_advantage, matches_played_home, matches_played_away
            ]
            
            # Make predictions
            X_new = np.array([features])
            result_pred = class_model.predict(X_new)[0]
            result_proba = class_model.predict_proba(X_new)[0]
            goals_pred = reg_model.predict(X_new)[0]
            
            # Decode result
            result_decoded = result_encoder.inverse_transform([result_pred])[0]
            confidence = np.max(result_proba)
            
            predictions_data.append({
                'home_team': home_team,
                'away_team': away_team,
                'predicted_result': result_decoded,
                'predicted_home_goals': max(0, round(goals_pred[0])),
                'predicted_away_goals': max(0, round(goals_pred[1])),
                'confidence': confidence,
                'home_win_prob': result_proba[np.where(result_encoder.classes_ == 'H')[0][0]],
                'draw_prob': result_proba[np.where(result_encoder.classes_ == 'D')[0][0]],
                'away_win_prob': result_proba[np.where(result_encoder.classes_ == 'A')[0][0]]
            })
        
        predictions_df = pd.DataFrame(predictions_data)
        
        # Display predictions
        print(f"Predictions for {len(predictions_df)} matches:")
        print("-" * 80)
        
        for _, row in predictions_df.iterrows():
            print(f"Home: {row['home_team']}, Away: {row['away_team']} â†’ "
                  f"Result: {row['predicted_result']}, "
                  f"Score: {row['predicted_home_goals']}-{row['predicted_away_goals']} "
                  f"(Confidence: {row['confidence']:.2%})")
            print(f"   Probabilities - H: {row['home_win_prob']:.2%}, "
                  f"D: {row['draw_prob']:.2%}, A: {row['away_win_prob']:.2%}")
            print()
        
        return predictions_df
        
    except Exception as e:
        print(f"âŒ Error making predictions: {str(e)}")
        return pd.DataFrame()

def save_results_to_excel(all_results, filename="football_predictions_complete.xlsx"):
    """
    Save comprehensive results to Excel with multiple sheets
    
    Args:
        all_results: DataFrame with all predictions
        filename: Excel filename to save
    """
    print("\n" + "="*50)
    print("ğŸ“Š SAVING RESULTS TO EXCEL")
    print("="*50)
    
    try:
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            # Sheet 1: All predictions
            all_results.to_excel(writer, sheet_name='All_Predictions', index=False)
            
            # Sheet 2: High confidence predictions (>80%)
            high_conf = all_results[all_results['prediction_confidence'] > 0.8]
            high_conf.to_excel(writer, sheet_name='High_Confidence', index=False)
            
            # Sheet 3: Correct predictions
            correct_preds = all_results[all_results['result'] == all_results['predicted_result']]
            correct_preds.to_excel(writer, sheet_name='Correct_Predictions', index=False)
            
            # Sheet 4: Summary statistics
            summary_stats = {
                'Metric': [
                    'Total Matches',
                    'Overall Accuracy',
                    'High Confidence Accuracy',
                    'Home Win Accuracy',
                    'Away Win Accuracy', 
                    'Draw Accuracy',
                    'Exact Score Matches',
                    'Average Confidence'
                ],
                'Value': [
                    len(all_results),
                    f"{(all_results['result'] == all_results['predicted_result']).mean():.2%}",
                    f"{(high_conf['result'] == high_conf['predicted_result']).mean():.2%}" if len(high_conf) > 0 else "N/A",
                    f"{((all_results['result'] == 'H') & (all_results['predicted_result'] == 'H')).sum() / (all_results['result'] == 'H').sum():.2%}",
                    f"{((all_results['result'] == 'A') & (all_results['predicted_result'] == 'A')).sum() / (all_results['result'] == 'A').sum():.2%}",
                    f"{((all_results['result'] == 'D') & (all_results['predicted_result'] == 'D')).sum() / (all_results['result'] == 'D').sum():.2%}",
                    ((all_results['home_goals'] == all_results['predicted_home_goals']) & 
                     (all_results['away_goals'] == all_results['predicted_away_goals'])).sum(),
                    f"{all_results['prediction_confidence'].mean():.2%}"
                ]
            }
            
            summary_df = pd.DataFrame(summary_stats)
            summary_df.to_excel(writer, sheet_name='Summary_Stats', index=False)
            
            # Sheet 5: Team performance
            team_stats = []
            teams = set(all_results['home_team'].unique()) | set(all_results['away_team'].unique())
            
            for team in teams:
                home_matches = all_results[all_results['home_team'] == team]
                away_matches = all_results[all_results['away_team'] == team]
                
                home_correct = (home_matches['result'] == home_matches['predicted_result']).sum()
                away_correct = (away_matches['result'] == away_matches['predicted_result']).sum()
                
                total_matches = len(home_matches) + len(away_matches)
                total_correct = home_correct + away_correct
                
                if total_matches > 0:
                    team_stats.append({
                        'Team': team,
                        'Total_Matches': total_matches,
                        'Correct_Predictions': total_correct,
                        'Accuracy': f"{total_correct/total_matches:.2%}",
                        'Home_Matches': len(home_matches),
                        'Away_Matches': len(away_matches)
                    })
            
            team_df = pd.DataFrame(team_stats).sort_values('Accuracy', ascending=False)
            team_df.to_excel(writer, sheet_name='Team_Performance', index=False)
        
        print(f"âœ… Results saved to '{filename}' with 5 sheets:")
        print("   ğŸ“‹ All_Predictions - Complete prediction results")
        print("   ğŸ¯ High_Confidence - Predictions with >80% confidence")
        print("   âœ… Correct_Predictions - Successfully predicted matches") 
        print("   ğŸ“Š Summary_Stats - Overall performance metrics")
        print("   ğŸ‘¥ Team_Performance - Per-team prediction accuracy")
        
    except Exception as e:
        print(f"âŒ Error saving to Excel: {str(e)}")

def demonstrate_model_usage():
    """
    Demonstrate how to use saved models for new predictions
    """
    print("\n" + "="*50)
    print("ğŸ¯ DEMONSTRATION: USING SAVED MODELS")
    print("="*50)
    
    # Try to load existing models
    loaded_models = load_models_and_encoders()
    
    if loaded_models[0] is not None:  # If models loaded successfully
        class_model, reg_model, home_encoder, away_encoder, result_encoder, scaler, metadata = loaded_models
        
        # Example predictions for new matches
        print("\nğŸ“ Example: Predicting upcoming matches")
        sample_matches = [
            ("Liverpool", "Manchester United"),
            ("Arsenal", "Chelsea"),
            ("Manchester City", "Tottenham Hotspur")
        ]
        
        home_teams = [match[0] for match in sample_matches]
        away_teams = [match[1] for match in sample_matches]
        
        new_predictions = predict_new_matches(
            home_teams, away_teams, class_model, reg_model, 
            home_encoder, away_encoder, result_encoder
        )
        
        if not new_predictions.empty:
            # Save new predictions to Excel
            new_predictions.to_excel('new_match_predictions.xlsx', index=False)
            print("ğŸ’¾ New predictions saved to 'new_match_predictions.xlsx'")
    
    else:
        print("âš ï¸  No saved models found. Train the model first using main() function.")

def main():
    """
    Enhanced main function with model saving and Excel output
    """
    try:
        print("ğŸš€ ADVANCED FOOTBALL PREDICTION SYSTEM WITH MODEL SAVING")
        print("=" * 60)
        
        # 1. Load and preprocess with advanced features
        X, y_class, y_reg, home_encoder, away_encoder, result_encoder, original_df = load_and_preprocess_data('dataset_ml.xlsx')
        
        # 2. Split with more training data (80/20 split)
        X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = split_data(
            X, y_class, y_reg, test_size=0.2
        )
        
        # 3. Train advanced ensemble models
        class_model, scaler = train_advanced_classification_model(X_train, y_class_train)
        reg_model = train_advanced_regression_model(X_train, y_reg_train)
        
        # 4. Evaluate models
        class_metrics = evaluate_classification_model(class_model, X_test, y_class_test, result_encoder)
        reg_metrics = evaluate_regression_model(reg_model, X_test, y_reg_test)
        
        # 5. Make predictions for ALL matches
        all_results = make_predictions_for_all_matches(
            class_model, reg_model, X, original_df, 
            home_encoder, away_encoder, result_encoder
        )
        
        # 6. Save models and encoders
        save_models_and_encoders(
            class_model, reg_model, home_encoder, away_encoder, result_encoder, scaler
        )
        
        # 7. Save comprehensive results to Excel
        save_results_to_excel(all_results)
        
        # 8. Demonstrate model usage
        demonstrate_model_usage()
        
        print("\n" + "="*60)
        print("ğŸ¯ COMPLETE ANALYSIS WITH MODEL SAVING FINISHED!")
        print("="*60)
        print(f"ğŸ“ˆ Final Accuracy: {class_metrics['accuracy']:.1%}")
        print(f"ğŸ“ Models saved in 'football_models/' directory")
        print(f"ğŸ“Š Results saved in 'football_predictions_complete.xlsx'")
        
        if class_metrics['accuracy'] >= 0.85:
            print("ğŸ† TARGET ACHIEVED: 85%+ accuracy reached!")
        else:
            print(f"ğŸ“ˆ Current accuracy: {class_metrics['accuracy']:.1%} - Close to target!")
        
        return all_results
        
    except FileNotFoundError:
        print("âŒ Error: Could not find 'dataset_ml.xlsx'. Please ensure the file exists.")
    except Exception as e:
        print(f"âŒ Error: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # Run the complete pipeline
    results = main()
    
    # Additional demonstration of loading and using saved models
    print("\n" + "="*60)
    print("ğŸ”„ BONUS: DEMONSTRATING MODEL RELOADING")
    print("="*60)
    
    # This shows how you would use the models in a separate script
    loaded_models = load_models_and_encoders()
    if loaded_models[0] is not None:
        print("âœ… Models successfully reloaded and ready for new predictions!")